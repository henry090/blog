---
title: "Tensorflow Hub + R + State-of-the-art models" 
categories:
  - Tensorflow Hub
  - State of the art
description: |
  Tensorflow Hub ilə "State-of-the-art" modellərindən istifadə edərək yüksək dəqiqliyə malik olan modellər yaradın (NLP, Image, Video and etc.). Məsələn, mətnlər üzərində sentiment analizi aparmaq üçün heç bir "preprocess" mərhələlərinə ehtiyac yoxdur. "State-of-the-art" modelinin özü mətni avtomatik emal edərək hesablamanı həyata keçirəcəkdir. Bütün əməliyyatları 25 sətr kod ilə icra etmək mümkündür.
resources:
  exclude:
preview: tf_hub.jpg
author:
  - name: Turqut Abdullayev
    affiliation: Access Bank /  QSS 
date: 08-25-2019
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


&nbsp; &nbsp; &nbsp; &nbsp; __[Tensorflow Hub](https://tfhub.dev/)__ Maşın Öyrənməsi modullarını özündə əks etdirən bir sistemdir. Və bu sistemdə modullar *Tensorflow* qrafiklərini, çəkilərini və digər komponentlərini əks etdirməklə yanaşı kiçik həcmli məlumatlar üzərində işləməyi, ümümiləşdirməni və sürətlı əməliyyat prosesini istifadəçilərə təqdim edir.

Ümumiyyətlə, kiçik həcmli məlumatlar ümumiləşdirməni həyata keçirməkdə böyük çətinliklər yaradır. Tf Hub vasitəsilə bu problem aradan qalxır, çünki əvvəlcədən yaradılmış model N sayda data-yla işlədiyi üçün, istifadəçi tərəfindən modelə ötürülən yətərsiz məlumat, modelə heç bir çətinlik yarada bilmir. Əksinə "State-of-the-art" modelini baza kimi istifadə edərək, öz datamızı modelin arxasına əlavə edərək, məsələn sentiment analizini rahatlıqla yerinə yetirə bilərik.

Bu postda ```Kaggle```-da 1 il bunnan qabaq keçirilmiş mətn klassifikasiyası case-i ilə işləyəcəyik. Mahiyət isə 6 kateqoriya üzrə qruplaşdırılan mətnləri düzgün ayıra bilən model yaratmaqdır. Məsələn aşağıdakı şərh bir neçə qrupa aid edilib.

> https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data

```{r eval=T,echo=F,layout="l-body-outset"}
library(knitr)
kable(data.table::fread('comment_56.csv'))
```


Data __159 K__ ibarət olan şərhlərdən ibarətdir. Bu rəqəm model yaratmaq üçün olduqca azdır . Belə hallarda manual olaraq "State-of-the-art" olan modelin nəticəsini R-a yükləyərək yeni modeli onun üzərində yaratmaq olar. Amma bu proses həqiqətəndə çox manual addımlar tələb edir və istifadəçinin vaxtını, işini çoxaldır. 

Bu tipli çətinlikləri aradan qaldırmaq üçün __[Tensorflow Hub](https://tfhub.dev/)__ sistemi yaradılmışdır. Və bu sistem vasitəsilə aşağıdakı problemləri həll etmək olar:

## Text
- Embedding

## Image
- Augmentation
- Classification
- Feature Vector
- Generator
- Object Detection
- Style Transfer
- Other

## Video
- Classification


İndi isə həmin dataset üzərində 25 sətr kodla modeli yarada bilərik. Qeyd edim ki, mətnləri preprocess etməyə ehtiyac yoxdur. Preprocess deyəndə mətndəki lazımsız simvolları təmizləyərək, indekslərə çevirmək, daha sonra isə hər bir sözün çəkisini "State-of-the-art" modelinin nəticəsindən çıxan vektorlarla birləşdirməyə ehtiyac olmaması deməkdir. Model bütün addımları özü edəcəkdir.

__Nümunə:__

```{r, echo=T,eval=F}
# cümlə çox çirklidir və bəzi sözlər böyük hərflə yazılmışdır
"''''hello', something ,,,went wRong @@after ...meetinG"
```

Datanı manual təmizləyərək, sözləri Glove modelindəki vekorlarla əvəz etməliyik. Məsələn.

```{r, echo=T,eval=F}
"hello" "something" "went" "wrong" "after" "meeting"
"0.567" "0.267"     "0.96"  "0.1"   "0.311" "0.532"
```

Bütün bu addımları etməyə ehtiyac yoxdur. Tensorflow Hub-dakı modellər avtomatik olaraq bu addımları sizin yerinizə edəcəkdir.

```{r, echo=T,eval=F}
# install all the necessary libraries
tensorflow::install_tensorflow(version = "2.0.0-beta1")

remotes::install_github("rstudio/tfdatasets")
remotes::install_github("rstudio/keras")

# open them
library(keras)
library(tfhub)
# install python tf_hub
tfhub::install_tfhub()

# import dataset. 1st Download it from 
# https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data
# Then open train.csv dataset
comments <- data.table::fread('/Users/turgutabdullayev/Downloads/train_.csv')

# Load pre-trained model with 128 dim
embeddings <- layer_hub(
  handle = "https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1",
  trainable = F
)

# input layer for text
input <- layer_input(shape = shape(), dtype = "string")

# import model to your acrhitecture
output = embeddings(input) 

# lambda function for reshaping 1d input of tf hub model into 3d due to gru_layer
output = layer_lambda(f = function(x) {(k_reshape(x,shape = list(-1,1,128)))})(c(output))

# apply bidirectional + layer gru to your model
output =output %>%  bidirectional(layer_gru(units = 80,return_sequences = T)) 

# functional api = max_pool + average_pooling
max_pool = output %>% layer_global_max_pooling_1d()
ave_pool = output %>% layer_global_average_pooling_1d()

# concatenate them via concatenate function
outp = layer_concatenate(list(ave_pool, max_pool)) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 6, activation = "sigmoid")

# gather into final model via keras_model
model <- keras_model(input, outp)

# compile model
model %>% compile(loss = "binary_crossentropy", optimizer = "adam", 
metrics = list(tf$keras$metrics$AUC()))

# and run for 1 epoch (is enough!!!)
model %>% fit(epochs=1, x = comments$comment_text, 
y = as.matrix(comments[,-c(1:2)]), validation_split = 0.2)

```


Google tərəfindən ```pre-trained``` edilmiş ```embedding language``` modeli 128 dimension-dan ibarətdir. Və bu model bizə AUC göstəricisini 0.97-yə kimi verir (olduqca yüksək nəticədir). Glove (0.98) ilə müqayisədə yalnız 1 % aşağıdır (based on Kaggle notebooks where Glove has 300 dimensions).



